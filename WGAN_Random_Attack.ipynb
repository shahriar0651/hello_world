{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.autograd.variable import Variable\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn.functional as Func\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "import tqdm\n",
    "import glob\n",
    "\n",
    "from functions.functions_for_dataload import *\n",
    "from functions.fnc import *\n",
    "\n",
    "######### Generating new samples ###########\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "\n",
    "today = date.today()\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "numOfBuses = 14\n",
    "numOfLines = 20\n",
    "attacked_Bus = 5\n",
    "\n",
    "num_Of_attacked_buses = attacked_Bus\n",
    "\n",
    "noise_dimension = numOfBuses\n",
    "z_dim = noise_dimension\n",
    "numOfZ = numOfBuses + 2* numOfLines\n",
    "numOfStates = numOfBuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f\"Bus Data\\\\IEEE_{numOfBuses}.xlsx\"\n",
    "\n",
    "# Load data into Dataframes\n",
    "bus_data_df = pd.read_excel (file_name, sheet_name = \"Bus\")\n",
    "line_data_df = pd.read_excel (file_name, sheet_name = \"Branch\")\n",
    "# update the index from 1 to number of elements\n",
    "bus_data_df.set_index(pd.Series(range(1, numOfBuses+1)), inplace = True)\n",
    "line_data_df.set_index(pd.Series(range(1, numOfLines+1)), inplace = True)\n",
    "\n",
    "# number of lines and measurements\n",
    "numOfLines = line_data_df.shape[0]\n",
    "numOfZ = numOfBuses + numOfLines * 2\n",
    "W_list = (numOfZ + 1)*[1]\n",
    "\n",
    "############################################################\n",
    "# Loading Topology Data and Measurement Data\n",
    "try:\n",
    "    topo_mat = pd.read_excel(file_name, sheet_name = \"Topology Matrix\")\n",
    "    line_data  = pd.read_excel(file_name, sheet_name = \"Line Data\")\n",
    "    print(\"Topology Matrix Loaded!\")\n",
    "except:\n",
    "    print(\"Generating Topology Matrix...\")\n",
    "    topo_mat, line_data = generate_topology_matrix(numOfBuses, numOfLines, line_data_df, file_name)\n",
    "\n",
    "Topo = line_data.values.astype(int) #Another name\n",
    "###############################################################\n",
    "# Loading Topology Data and Measurement Data\n",
    "try:\n",
    "    Z_msr_org = pd.read_excel(file_name, sheet_name = \"Measurement Data\")\n",
    "    bus_data = pd.read_excel(file_name, sheet_name = \"Bus Data\")\n",
    "    print(\"Measurement Data Loaded!\")\n",
    "except:\n",
    "    print(\"Generating Measurement Data...\")\n",
    "    Z_msr_org, bus_data = generate_Z_msr_org(numOfBuses, numOfLines, bus_data_df, topo_mat, file_name)\n",
    "\n",
    "# Adding IDs and Reported columns\n",
    "Z_msr_org.insert(0, 'ID', list(Z_msr_org.index.values))\n",
    "Z_msr_org.insert(1, 'Reported', [1]* (numOfZ+1))\n",
    "###############################################################\n",
    "#************  reading network topo data **************\n",
    "fnetname = f'Bus Data//Net_Topo_{numOfBuses}.txt'\n",
    "try: netdata = open(fnetname).readlines() \n",
    "except: \n",
    "    print(\"File Does not exist!\")\n",
    "################################################################\n",
    "\n",
    "# Load Attack Data, otherwise generate attack data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_others(numOfBuses, numOfLines, num_Of_attacked_buses, data, Topo):\n",
    "    data = pd.DataFrame(data)\n",
    "    numOfZ = numOfBuses + 2* numOfLines\n",
    "    num_Of_attack = int (data.shape[0]/numOfZ)\n",
    "    # Preprocessing data\n",
    "\n",
    "    false_Data = pd.DataFrame([])\n",
    "    combination_Data_Sensor = pd.DataFrame([])\n",
    "    combination_Data_Cluster = pd.DataFrame([])\n",
    "\n",
    "    start_indx = 0\n",
    "    end_indx = start_indx + numOfZ + 1\n",
    "\n",
    "    progress = tqdm.tqdm(total=num_Of_attack, desc='Done', position=0)\n",
    "    while (end_indx <= data.shape[0]):\n",
    "        progress.update(1)\n",
    "        attack_data = data.iloc[start_indx + 1 : end_indx, 2]\n",
    "        combination_data = data.iloc[start_indx + 1 : end_indx, 1].astype(int)\n",
    "\n",
    "        sensor_ids = np.where(combination_data.values == 1)[0]+1\n",
    "        if len(sensor_ids) > 0:        \n",
    "            cluster_maping,_ =  assignClusterID (sensor_ids, Topo, NumOfBuses=numOfBuses, NumOfLines = numOfLines)\n",
    "            combination_Data_Cluster = combination_Data_Cluster.append(pd.DataFrame(cluster_maping, index = None, columns = None).T)\n",
    "\n",
    "            false_Data = false_Data.append(pd.DataFrame(attack_data.values, index = None, columns = None).T)\n",
    "            combination_Data_Sensor = combination_Data_Sensor.append(pd.DataFrame(combination_data.values, index = None, columns = None).T)\n",
    "\n",
    "        start_indx += numOfZ+1\n",
    "        end_indx = start_indx + numOfZ + 1\n",
    "        #print(start_indx, end_indx)\n",
    "    false_Data.index = range(1, false_Data.shape[0]+1)\n",
    "    false_Data.columns = range(1, false_Data.shape[1]+1)\n",
    "    combination_Data_Sensor.index = range(1, combination_Data_Sensor.shape[0]+1)\n",
    "    combination_Data_Sensor.columns = range(1, combination_Data_Sensor.shape[1]+1)\n",
    "\n",
    "    combination_Data_Cluster.index = range(1, combination_Data_Cluster.shape[0]+1)\n",
    "    combination_Data_Cluster.columns = range(1, combination_Data_Cluster.shape[1]+1)\n",
    "\n",
    "\n",
    "    false_Data.to_excel(f'Attack Data//false_Data_{numOfBuses}_{numOfLines}_{num_Of_attacked_buses}.xlsx', engine='xlsxwriter', index = True) \n",
    "    combination_Data_Sensor.to_excel(f'Attack Data//combination_Data_Sensor_{numOfBuses}_{numOfLines}_{num_Of_attacked_buses}.xlsx', engine='xlsxwriter', index = True) \n",
    "    combination_Data_Cluster.to_excel(f'Attack Data//combination_Data_Cluster_{numOfBuses}_{numOfLines}_{num_Of_attacked_buses}.xlsx', engine='xlsxwriter', index = True) \n",
    "    return false_Data, combination_Data_Sensor, combination_Data_Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Attack Data, otherwise generate attack data\n",
    "\n",
    "try:\n",
    "    file_Name_ = \"Attack_Space_\"+str(numOfBuses)+\"_\"+str(numOfLines)+\"_\"+str(attacked_Bus)+\".csv\"\n",
    "    Attack_Data = np.genfromtxt(\"Attack Data//\"+file_Name_, delimiter=',')\n",
    "    print(\"Attack data loaded!\")\n",
    "    file_name = f\"IEEE_{numOfBuses}.xlsx\"\n",
    "    false_Data = pd.read_excel(f'Attack Data//false_Data_{numOfBuses}_{numOfLines}_{attacked_Bus}.xlsx', index_col=0)\n",
    "    combination_Data_Sensor = pd.read_excel(f'Attack Data//combination_Data_Sensor_{numOfBuses}_{numOfLines}_{num_Of_attacked_buses}.xlsx', index_col=0)\n",
    "    combination_Data_Cluster = pd.read_excel(f'Attack Data//combination_Data_Cluster_{numOfBuses}_{numOfLines}_{num_Of_attacked_buses}.xlsx', index_col=0)\n",
    "    # combination_Data = pd.read_excel(f'Attack Data//combination_Data_{numOfBuses}_{numOfLines}_{attacked_Bus}.xlsx', index_col=0)\n",
    "\n",
    "except:\n",
    "    print(\"Attack Data is missing! Generating attack data!\")\n",
    "    current_path = os.getcwd()\n",
    "    Attack_Data = generate_attackdata(numOfBuses, numOfLines, line_data, attacked_Bus, current_path)\n",
    "    \n",
    "    false_Data, combination_Data_Sensor, combination_Data_Cluster = generate_others(\n",
    "        numOfBuses, numOfLines, attacked_Bus, Attack_Data, Topo)\n",
    "    \n",
    "numOfAttacks = int (Attack_Data.shape[0]/(numOfZ+1))\n",
    "print(\"numOfAttacks: \", numOfAttacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination_Data = combination_Data_Sensor.copy()\n",
    "\n",
    "test_size = 0.25\n",
    "# Scaling the data\n",
    "stdscaler = StandardScaler()\n",
    "X_org = stdscaler.fit_transform(false_Data) \n",
    "\n",
    "#splitting train and test data\n",
    "false_Data_Train, false_Data_Test = train_test_split(false_Data, test_size = test_size, random_state=42)\n",
    "X_Train, X_Test = train_test_split(X_org, test_size = test_size, random_state=42)\n",
    "\n",
    "combination_Data_Train, combination_Data_Test =  train_test_split(combination_Data, test_size = test_size, random_state=42)\n",
    "\n",
    "print(\"Split done!\")\n",
    "print(\"Total training data: \", false_Data_Train.shape[0])\n",
    "\n",
    "data = [x_i for x_i in X_Train]\n",
    "class Data_Loader():\n",
    "    \n",
    "    def __init__(self,data_list):       \n",
    "        self.data=data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        sample_tensor = Tensor(sample).float()\n",
    "#         label = self.data[index][1]\n",
    "        return sample_tensor\n",
    "\n",
    "dataSet=Data_Loader(data)\n",
    "data_loader = DataLoader(dataset=dataSet, batch_size = 64, shuffle= True)\n",
    "print(\"Data Loader Created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = \"WGAN_WC\"\n",
    "model_Train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if numOfBuses == 14:\n",
    "    G_nodes = [z_dim, 20, 40, numOfZ]\n",
    "    D_nodes = [numOfZ, 25, 10, 1]\n",
    "    \n",
    "elif numOfBuses == 30:\n",
    "    G_nodes = [z_dim, 56, 80, numOfZ]\n",
    "    D_nodes = [numOfZ, 64, 32, 1]\n",
    "\n",
    "elif numOfBuses == 57:\n",
    "    G_nodes = [z_dim, 80, 150, numOfZ]\n",
    "    D_nodes = [numOfZ, 100, 50, 1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Model == 'WGAN_WC':\n",
    "    # GeneratorNet\n",
    "    G = torch.nn.Sequential(\n",
    "        nn.Linear(G_nodes[0], G_nodes[1]),\n",
    "        nn.ReLU(True),\n",
    "    #     nn.Dropout(0.10),\n",
    "\n",
    "        nn.Linear(G_nodes[1], G_nodes[2]),\n",
    "        nn.ReLU(True),\n",
    "    #     nn.Dropout(0.10),\n",
    "        nn.Linear(G_nodes[2], G_nodes[3])\n",
    "    )\n",
    "\n",
    "    # DiscriminatorNet\n",
    "    D = torch.nn.Sequential(\n",
    "        nn.Linear(D_nodes[0], D_nodes[1]),\n",
    "        nn.ReLU(True),\n",
    "    #     nn.Dropout(0.15),\n",
    "\n",
    "        nn.Linear(D_nodes[1], D_nodes[2]),\n",
    "        nn.ReLU(True),\n",
    "    #     nn.Dropout(0.15),\n",
    "\n",
    "        nn.Linear(D_nodes[2], D_nodes[3])\n",
    "    #     nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "    # reset_grad\n",
    "    def reset_grad():\n",
    "        G.zero_grad()\n",
    "        D.zero_grad()\n",
    "\n",
    "    def noise(size):\n",
    "        n = Variable(torch.randn(size, z_dim))\n",
    "        return n\n",
    "    \n",
    "    def train_discriminator(optimizer, real_data, fake_data):\n",
    "        \n",
    "        # predict on the real and fake data\n",
    "        D_real = D(real_data)\n",
    "        D_fake = D(fake_data)\n",
    "        \n",
    "        # Calculate loss\n",
    "        D_loss = -(torch.mean(D_real) - torch.mean(D_fake))\n",
    "\n",
    "        D_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #Weight clipping\n",
    "        for p in D.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "#             #reset gradient\n",
    "        reset_grad()\n",
    "        # Return error and predictions for real and fake inputs\n",
    "        return D_loss\n",
    "\n",
    "    def train_generator(optimizer, fake_data):\n",
    "        # Reset gradients\n",
    "#         reset_grad()\n",
    "        # Sample noise and generate fake data\n",
    "        D_fake = D(fake_data)\n",
    "        # Calculate error and backpropagate\n",
    "        G_loss = -torch.mean(D_fake)\n",
    "        G_loss.backward()\n",
    "        # Update weights with gradients\n",
    "        optimizer.step()\n",
    "        # Return error\n",
    "        reset_grad()\n",
    "        return G_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_harmonizer(optimizer, sample):\n",
    "    sample = stdscaler.inverse_transform(sample.data)\n",
    "    sample = torch.from_numpy(sample)\n",
    "    sample_est = H_tensor@(torch.pinverse(H_tensor.T@H_tensor)@H_tensor.T)@sample.reshape(numOfZ,1).flatten()\n",
    "    #sample = torch.from_numpy(sample.flatten())\n",
    "    sample =sample.flatten()\n",
    "\n",
    "    sample = Variable(sample.data, requires_grad=True)\n",
    "    sample_est = Variable(sample_est.data, requires_grad=True)\n",
    "    G_loss_MSE = alpha * MSE_loss(sample_est, sample)\n",
    "    G_loss_MSE.backward()\n",
    "    optimizer.step()\n",
    "    #reset gradient\n",
    "#     reset_grad()\n",
    "    optimizer.zero_grad()\n",
    "    return G_loss_MSE\n",
    "\n",
    "def noise(size):\n",
    "    '''\n",
    "    Generates a 1-d vector of gaussian sampled random values\n",
    "    '''\n",
    "    n = Variable(torch.randn(size, noise_dimension))\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false_Data_Test.hist()\n",
    "def plot_corr():\n",
    "    \n",
    "    test_samples = 1000\n",
    "    false_data_dist = stdscaler.inverse_transform(G(noise(test_samples)).detach().numpy())\n",
    "    false_data_dist = pd.DataFrame(false_data_dist, columns = range(1,numOfZ+1), index = range(1,test_samples+1))\n",
    "    fig, ax = plt.subplots(figsize=(5,4))         \n",
    "    sns.heatmap(false_data_dist.corr(), annot=False, linewidths=0, ax=ax, center = 0, square = True)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(f'Evaluation//'+Model+f'//{numOfBuses}//{Model}_Heat_Map_{epoch}.jpg', dpi = 250)\n",
    "    plt.savefig(f'Evaluation//'+Model+f'//{numOfBuses}//{Model}_Heat_Map_{epoch}.jpg', dpi = 250)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def eval_model():\n",
    "    print(\"epoch: \", epoch,'\\n\\n')\n",
    "    ######\n",
    "    # Ploting the laerning curve\n",
    "    plt.close()\n",
    "    plt.figure(0)\n",
    "    plt.plot(d_errs, color = 'r')\n",
    "    plt.plot(g_errs, color = 'g')\n",
    "    plt.show()\n",
    "    # plotting correlation\n",
    "    plot_corr()\n",
    "    \n",
    "def save_model():\n",
    "    \n",
    "    time = str(datetime.now()).split(\".\")[0]\n",
    "    time = time.replace(\":\",\"-\")\n",
    "\n",
    "    torch.save(G, f'Models//'+Model+f'//{numOfBuses}//{Model}_{attacked_Bus}_{cond_type}_generator_{epoch}_'+ str(time) + \".pb\")\n",
    "    torch.save(D, f'Models//'+Model+f'//{numOfBuses}//{Model}_{attacked_Bus}_{cond_type}_discriminator_{epoch}_'+ str(time) + \".pb\")\n",
    "\n",
    "    #Saving the learning curve\n",
    "    plt.figure(0)\n",
    "    plt.plot(d_errs, color = 'r', label= 'd_errs')\n",
    "    plt.plot(g_errs, color = 'g', label= 'g_errs')\n",
    "    plt.xlabel(\"Number of epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Training curve of {Model}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f'Models//'+Model+f'//{numOfBuses}//{Model}_{attacked_Bus}_{cond_type}_Learning_Curve_{epoch}_' + str(time) +'.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################    Copying the original Data ###########################\n",
    "print('Checking State Estimation and Bad Data Detector on clean data') \n",
    "H_org = topo_mat.values.copy() \n",
    "Z_org = Z_msr_org.values.copy()\n",
    "Z_mat = Z_org.copy()\n",
    "Threshold_min = 2\n",
    "Threshold_max = 2\n",
    "W_list = (numOfZ + 1)*[1]\n",
    "##################################################################################\n",
    "# Running the State Estimation and Bad Data Detection Algorithm on actual data\n",
    "States_init, Z_est_init, Z_mat_init, M_Noise_actu, Noisy_index_actu, fullRank, Threshold = SE_BDD_(\n",
    "    H_org.copy(), Z_org.copy(), W_list, Threshold_min, Threshold_max, Verbose = \"False\")\n",
    "if Noisy_index_actu.size == 0:\n",
    "    print(\"Working fine!\")\n",
    "else: print(\"Got some issue!!!!!!\")\n",
    "    \n",
    "H_tensor = torch.from_numpy(H_org[1:,:]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "\n",
    "cond_type  = 0\n",
    "\n",
    "MSE_loss = nn.MSELoss()\n",
    "num_epochs = 25000\n",
    "epoch = num_epochs-1\n",
    "lr = 5e-5\n",
    "disRepaet = 5\n",
    "max_harmonize = 64\n",
    "# max_harmonize = 0\n",
    "alpha = lr\n",
    "\n",
    "#List to save the loss\n",
    "g_errs = []\n",
    "d_errs = []\n",
    "mse_errs = []\n",
    "loss = nn.BCELoss()\n",
    " \n",
    "if Model == 'WGAN_WC' and model_Train == True:\n",
    "    \n",
    "    # Optimizer functions\n",
    "    g_optimizer = optim.RMSprop(G.parameters(), lr = lr, momentum= 0.9)\n",
    "    d_optimizer = optim.RMSprop(D.parameters(), lr = lr, momentum= 0.9)\n",
    "    \n",
    "    progress = tqdm.tqdm(total=num_epochs, desc='Epoch', position = 0)\n",
    "\n",
    "    # starting the epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        #print(\"Epoch: \", epoch)\n",
    "        progress.update(1)\n",
    "\n",
    "        # Starting Discriminator training\n",
    "        for _ in range(disRepaet):\n",
    "            # Sample data\n",
    "            for n_batch, real_batch in enumerate(data_loader):\n",
    "                N = real_batch.size(0)   \n",
    "                real_data = Variable(real_batch)\n",
    "                # Generate fake data and detach \n",
    "                fake_data = G(noise(N)).detach()\n",
    "                # Train D\n",
    "                D_loss = train_discriminator(d_optimizer, real_data, fake_data)\n",
    "                #reset gradient\n",
    "\n",
    "\n",
    "        # Starting Generator training\n",
    "        for n_batch, real_batch in enumerate(data_loader):\n",
    "            N = real_batch.size(0)   \n",
    "            # Generate fake data\n",
    "            fake_data = G(noise(N))\n",
    "            # Train G\n",
    "            G_loss = train_generator(g_optimizer, fake_data)\n",
    "\n",
    "        #Starting harmonizer training\n",
    "#         if epoch > 0 :\n",
    "        for harmonize_repeat in range(max_harmonize):\n",
    "            fake_data = G(noise(1))\n",
    "            G_loss_MSE = train_harmonizer(g_optimizer, fake_data)\n",
    "            mse_errs.append(G_loss_MSE)       \n",
    "\n",
    "\n",
    "        d_errs.append(D_loss)\n",
    "        g_errs.append(G_loss)\n",
    "        \n",
    "        if (epoch) % 500 == 0:\n",
    "            # show and eval model generating a single sample\n",
    "            eval_model()\n",
    "            save_model()\n",
    "    ##################### Saving the model #######\n",
    "    save_model()\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "# # importing the latest models\n",
    "else:\n",
    "    Models_list = glob.glob(f'Models//'+Model+f'//{numOfBuses}//{Model}_{attacked_Bus}_{cond_type}_*_{epoch}_*.pb')\n",
    "    try:\n",
    "        Models_list.sort(reverse= True)\n",
    "        latest_time = Models_list[0].split(\"_\")[-1][0:-3]\n",
    "        print(\"latest_time:\", latest_time)\n",
    "        print(f'Models//'+Model+f'//{numOfBuses}//{Model}_{attacked_Bus}_{cond_type}_generator_{epoch}_'+ str(latest_time) + \".pb\")\n",
    "        G = torch.load(f'Models//'+Model+f'//{numOfBuses}//{Model}_{attacked_Bus}_{cond_type}_generator_{epoch}_'+ str(latest_time) + \".pb\")\n",
    "        D = torch.load(f'Models//'+Model+f'//{numOfBuses}//{Model}_{attacked_Bus}_{cond_type}_discriminator_{epoch}_'+ str(latest_time) + \".pb\")\n",
    "        print(\"Model Loaded\")\n",
    "    except:\n",
    "        print(\"Model not found!!!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 'final'\n",
    "test_samples = 10\n",
    "false_data_dist = stdscaler.inverse_transform(G(noise(test_samples)).detach().numpy())\n",
    "false_data_dist = pd.DataFrame(false_data_dist, columns = range(1,numOfZ+1), index = range(1,test_samples+1))\n",
    "fig, ax = plt.subplots(figsize=(5,4))         \n",
    "sns.heatmap(false_data_dist.corr(), annot=False, linewidths=0, ax=ax, center = 0, square = True)\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.savefig('Evaluation//'+Model+f'//{numOfBuses}//Heat_Map_{epoch}.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,4))         \n",
    "sns.heatmap(false_Data.corr(), annot=False, linewidths=0, ax=ax, center = 0, square = True)\n",
    "plt.tight_layout()\n",
    "plt.grid(False)\n",
    "plt.savefig('Evaluation//'+Model+f'//{numOfBuses}//Training_Heat_Map.jpg', dpi = 250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false_Data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.axes_style(\"whitegrid\")\n",
    "\n",
    "target_lines = numOfLines\n",
    "selected_lines = range(1, numOfLines+1)\n",
    "# print(selected_lines)\n",
    "\n",
    "fig, axList = plt.subplots(nrows=2, ncols=10)\n",
    "\n",
    "axList = axList.flatten()\n",
    "fig.set_size_inches(36,8)\n",
    "\n",
    "for ax, line in zip(axList, selected_lines):\n",
    "    ax.scatter(false_data_dist[line].values,false_data_dist[line+numOfLines].values, alpha = 0.5,  s= 150, marker= 'p', label = 'Generated')\n",
    "    ax.scatter(false_Data_Train[line].values,false_Data_Train[line+numOfLines].values, alpha = 0.5,  s= 150, marker= 'p', label = 'Training')\n",
    "\n",
    "    ax.set_title(f\"line: {line}\")\n",
    "    ax.set_aspect('equal')\n",
    "ax.legend(fontsize= 20)  \n",
    "# # Set common labels\n",
    "fig.text(0.5, 0.02, 'Forward line power flow', ha='center', va='center',fontsize= 40)\n",
    "fig.text(0.003, 0.5, 'Backward line power flow', ha='center', va='center', rotation='vertical', fontsize= 40)\n",
    "fig.text(0.5, 0.98, 'Correlation between forward and backward line power flow', ha='center', va='center', fontsize= 40)\n",
    "# fig.set_title(\"Forward and backward linepower flow\")\n",
    "plt.tight_layout()  \n",
    "# plt.savefig(f'Evaluation//'+Model+'//Linewise_Corr.pdf')\n",
    "\n",
    "plt.savefig('Evaluation//'+Model+f'//{numOfBuses}//Linewise_Corr.jpg', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.axes_style(\"whitegrid\")\n",
    "targets = random.sample(range(1,numOfZ), 5)\n",
    "\n",
    "for target in targets:\n",
    "    fig,ax = plt.subplots(figsize=(5,3.5))\n",
    "    ax = sns.kdeplot(false_Data_Train[target], shade = True, color = 'b',   label = '$\\mathcal{T}_{F}$')\n",
    "    plt.legend(loc = 'upper left', fontsize = '13')\n",
    "    plt.ylabel('Density (Traing)', fontsize = '15')\n",
    "    plt.xlabel(\"Injection Amount\", fontsize = '15')\n",
    "    plt.xticks (fontsize = '13')\n",
    "    plt.yticks (fontsize = '13')\n",
    "\n",
    "    ax2=ax.twinx()\n",
    "    ax2 = sns.kdeplot(false_data_dist[target], shade = True, color = 'r', linestyle = '--', label = '$\\mathcal{T}_{G}$')\n",
    "    plt.legend(loc = 'upper right', fontsize = '15')\n",
    "    plt.xlabel(\"Values\", fontsize = '15')\n",
    "    plt.ylabel('Density (Generated)', fontsize = '15')\n",
    "    plt.title(f'Distribution of Sensor : {target}', fontsize = '15')\n",
    "    plt.grid(True)\n",
    "    plt.xticks (fontsize = '13')\n",
    "    plt.yticks (fontsize = '13')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Evaluation//'+Model+f'//{numOfBuses}//{Model}_sensor_dist_{target}.jpg', dpi = 350)\n",
    "    plt.savefig('Evaluation//'+Model+f'//{numOfBuses}//{Model}_sensor_dist_{target}.pdf')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating more samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "def attack_estimate(H_mat, Z_mat):\n",
    "    import numpy as np\n",
    "    from numpy.random import seed\n",
    "    from numpy.random import randn\n",
    "    from numpy import mean\n",
    "    from numpy import std\n",
    "    \n",
    "#     Z_mat = Z_mat[Z_mat[:,0].argsort(kind='mergesort')]\n",
    "    #print(\"\\n **************  State Estimation ************* \\n\")\n",
    "    #takes H matrix Z measurements, Threshold and number of Equations as input\n",
    "    #print(\"Considering only the taken measurements\")\n",
    "    \n",
    "    Z_msr_init = Z_mat[Z_mat[:,1]==1][:,2]\n",
    "    \n",
    "    # considering only the corresponding columns in H   \n",
    "    \n",
    "    H_mat_init = H_mat[Z_mat[:,1]==1]\n",
    "\n",
    "    #printing the sizes of H, Rank (H) and Z\n",
    "    \n",
    "    Rank = np.linalg.matrix_rank(H_mat_init)\n",
    "    #print(\"Rank of Noisy: \" , Rank)\n",
    "    \n",
    "    Z_est = np.zeros(H_mat.shape[0])\n",
    "    \n",
    "    if Rank == H_mat.shape[1]:\n",
    "        \n",
    "        fullRank = True\n",
    "        # Estimating the states using WMSE estimator\n",
    "        #States_init = np.linalg.pinv(H_mat_init)@Z_msr_init\n",
    "        States_init = (np.linalg.inv(H_mat_init.T@H_mat_init)@H_mat_init.T)@Z_msr_init\n",
    "\n",
    "        # Estimating the measurement from the estimated states\n",
    "        Z_est = H_mat@States_init\n",
    "\n",
    "    else:\n",
    "        \n",
    "        #print(\"The H is not a full rank matrix !!\")\n",
    "        fullRank = False\n",
    "        States_init = 0\n",
    "        #Z_est = 0\n",
    "        P_Noise = 0\n",
    "        \n",
    "    return States_init, Z_est, fullRank\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Updated\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Threshold_min = 2\n",
    "# Threshold_max = 2\n",
    "\n",
    "# # Generated images\n",
    "# maxGen = 1000\n",
    "\n",
    "# # Sdt factors\n",
    "# std_factors = [0.00, 0.25, 0.50]\n",
    "\n",
    "# max_est_list = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# run_harmonizer = True\n",
    "\n",
    "# total_noisy_preds = []\n",
    "# total_distance = []\n",
    "# success_rates = []\n",
    "# false_Data_Gen_list = []\n",
    "# raw_Data_Gen_list = []\n",
    "# combination_Data_Gen_list = []\n",
    "# data_df_list = []\n",
    "# noisy_preds = []\n",
    "# # false_Data_Gen = pd.DataFrame([])\n",
    "# # data_df=  pd.DataFrame([])\n",
    "# distance = []\n",
    "# # success = [0, 0, 0, 0, 0]\n",
    "# # Generating new samples\n",
    "# # progress = tqdm.tqdm(total=len(std_factors), desc='Generation: ', position = 0)\n",
    "\n",
    "# # Generate more images\n",
    "# test_imagess =(G(noise(maxGen)))\n",
    "# #test_imagess = np.random.randn(maxGen, numOfZ)\n",
    "# test_imagess = test_imagess.data\n",
    "# samples = stdscaler.inverse_transform(test_imagess)\n",
    "# false_Data_Gen_Est = []\n",
    "# for max_est in max_est_list:\n",
    "#     success = [0]*len(std_factors)\n",
    "#     false_Data_Gen = pd.DataFrame([])\n",
    "#     data_df=  pd.DataFrame([])\n",
    "    \n",
    "#     for index, sample_init in enumerate(samples):\n",
    "#         #print(\"\\n\\nsample ID: \", index)\n",
    "\n",
    "#         #print(\"Generating Sample:\")\n",
    "#         sample_zero = sample_init.copy()\n",
    "\n",
    "#     #     if abs(sample_init).max()>500:\n",
    "#     #         print(\"Too_much: scaling the attack vector\")\n",
    "#     #         sample_init = sample_init*500/abs(sample_init).max()\n",
    "#     #     ############ Storing them as raw file ############\n",
    "\n",
    "\n",
    "#          ###### save the  raw data #######\n",
    "#         data_df[f'r_for_{index}'] = sample_init[0:20]\n",
    "#         data_df[f'r_bac_{index}'] = sample_init[20:40]\n",
    "#         data_df[f'r_bus_{index}'] = pd.Series(sample_init[40:].tolist())\n",
    "\n",
    "#         # Saving the raw data\n",
    "#         column_name = f'{index}_0'\n",
    "#         sample_df = pd.DataFrame(sample_init.tolist(), index = range(1, numOfZ+1), columns = [column_name])\n",
    "#         false_Data_Gen = false_Data_Gen.append(sample_df.T,  ignore_index = False)\n",
    "\n",
    "#         ###########################\n",
    "#         sample_mean = np.mean(sample_init)\n",
    "#         sample_std = np.std(sample_init)\n",
    "#         #print(\"Mean and Std:\", sample_mean, sample_std)\n",
    "\n",
    "#         # repeating for delZ_th_list\n",
    "#         for std_index, std_fact in enumerate(std_factors):\n",
    "#             #print(\"\\nstd_fact: \", std_fact)\n",
    "\n",
    "#             #setting up the threshold\n",
    "#             delZ_th = sample_std*std_fact\n",
    "#             #print(\"std_fact, delZ_th :\", std_fact, delZ_th)\n",
    "            \n",
    "#             # Repeating for muliple times\n",
    "#             for est in range(max_est+1):\n",
    "\n",
    "#                 #print(\"Estimation: \", est)\n",
    "\n",
    "#                 # Senitize the samples\n",
    "# #                 if run_harmonizer == True:\n",
    "#                 if max_est > 0:\n",
    "#                     Z_mat[1:, 2] = sample_init.copy()\n",
    "#                     States_org, Z_est_org, fullRank_org = attack_estimate(H_org.copy(), Z_mat.copy())\n",
    "#                     sample = Z_est_org[1:].copy()\n",
    "#             #         if est == 0:\n",
    "#                     dis = mean_absolute_error(sample_zero, Z_est_org[1:])\n",
    "#                         #dis  = np.mean(abs(sample_zero - Z_est_org[1:])) #/np.linalg.norm(Z_est_org[1:])*100\n",
    "#                 else:\n",
    "#                     sample = sample_init.copy()\n",
    "#                     dis = 0\n",
    "\n",
    "#                 # Removing noises\n",
    "#                 sample[abs(sample)<1] = 0\n",
    "#                 # Replacing with 0\n",
    "#                 sample[abs(sample)<delZ_th] = 0\n",
    "\n",
    "#                 ### Verifing the attack stealthyness\n",
    "#                 Z_mat[1:, 2] = Z_org[1:, 2] + sample\n",
    "#                 W_list = (numOfZ + 1)*[1]\n",
    "\n",
    "#                 # estimate the attack vectors\n",
    "#                 try:\n",
    "#                     States_init, Z_est_init, Z_mat_init, M_Noise_actu, Noisy_index_actu, fullRank, Threshold = SE_BDD_(\n",
    "#                     H_org.copy(), Z_mat.copy(), W_list, Threshold_min , Threshold_max , Verbose = \"False\")\n",
    "#                     #print(f\"\\nStep: {est}: Noise: \", Noisy_index_actu, \"Threshold:\", Threshold, \"fullRank:\", fullRank)\n",
    "#     #                 #save the noisy sensors in the fist estimation \n",
    "#     #                 if est == 0: noisy_preds.append(Noisy_index_actu.size)  \n",
    "\n",
    "#                 except:\n",
    "#                     #print(\"Error in estiamtion\")\n",
    "#                     break\n",
    "\n",
    "\n",
    "#                 #final_sample = sample.copy()\n",
    "\n",
    "#                 if  fullRank == False or np.linalg.norm(sample[0:40]) == 0: #abs(final_sample).max()> 1000 or\n",
    "#                     print(f\"\\nStep: {est}: Noise: \", Noisy_index_actu, \"Threshold:\", Threshold, \"fullRank:\", fullRank)\n",
    "#                     print(\"abs(sample).max()\", abs(sample).max(), sample.argmax())\n",
    "#                     break\n",
    "\n",
    "#                     ###########################\n",
    "\n",
    "#                 if Noisy_index_actu.size == 0:\n",
    "\n",
    "#                     distance.append(dis)\n",
    "#     #                 progress_mini.update(1)\n",
    "#                     success[std_index] += 1\n",
    "\n",
    "#                     #################### saving as the clean data \n",
    "#                     data_df[f'cf_{index}_{std_fact}'] = sample[0:20]\n",
    "#                     data_df[f'cb_{index}_{std_fact}'] = sample[20:40]\n",
    "#                     data_df[f'cc_{index}_{std_fact}'] = pd.Series(sample[40:].tolist())\n",
    "#                     ###################\n",
    "\n",
    "#                     ###########################\n",
    "#                     column_name = f'{index}_{std_index+1}'\n",
    "#                     sample_df = pd.DataFrame(sample.tolist(), index = range(1, numOfZ+1), columns = [column_name])\n",
    "#                     false_Data_Gen = false_Data_Gen.append(sample_df.T,  ignore_index = False)\n",
    "#                     ###########################\n",
    "\n",
    "#                     ###########################\n",
    "#                     #print(\"Clean\")\n",
    "#     #                 ###########################\n",
    "#                     break\n",
    "\n",
    "#     #             sample_init = Z_est_init[1:].copy() \n",
    "#                 #print(\"Repeating \", Noisy_index_actu.size)\n",
    "#                 sample_init = sample.copy()   \n",
    "\n",
    "#                 #saving sample and going for the next run\n",
    "#     #             sample = Z_est_init[1:].copy()\n",
    "\n",
    "#     false_Data_Gen_Est.append(false_Data_Gen)\n",
    "#     success_rates.append(success)\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_data_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine = 'WGAN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Updated\n",
    "machine = 'WGAN'\n",
    "#machine = 'RAND'\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "Threshold_min = 2\n",
    "Threshold_max = 2\n",
    "\n",
    "# Generated images\n",
    "if machine == 'WGAN':\n",
    "    samples = false_data_dist.values\n",
    "else:\n",
    "    samples = np.random.randint(250, size = false_data_dist.shape)\n",
    "\n",
    "# Sdt factors\n",
    "std_factors = [0.00, 0.25, 0.50, 0.75, 1.00]\n",
    "\n",
    "max_est_list = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# std_factors = [0.00, 0.25]\n",
    "# max_est_list = [0, 1]\n",
    "\n",
    "run_harmonizer = False\n",
    "\n",
    "total_noisy_preds = []\n",
    "total_distance = []\n",
    "success_rates = []\n",
    "false_Data_Gen_list = []\n",
    "raw_Data_Gen_list = []\n",
    "combination_Data_Gen_list = []\n",
    "data_df_list = []\n",
    "noisy_preds = []\n",
    "\n",
    "distance = []\n",
    "\n",
    "final_Data = pd.DataFrame([])\n",
    "false_Data_Gen_Est = []\n",
    "cond_Data_Gen_Est = []\n",
    "\n",
    "\n",
    "for max_est in max_est_list:\n",
    "    print(\"Max Estim:\", max_est)\n",
    "    success = [0]*len(std_factors)\n",
    "    false_Data_Gen = pd.DataFrame([])\n",
    "    cond_Data_Gen = pd.DataFrame([])\n",
    "    data_df=  pd.DataFrame([])\n",
    "    \n",
    "    for index, sample_init in enumerate(samples):\n",
    "        #print(\"\\n\\nsample ID: \", index)\n",
    "        #print(\"Generating Sample:\")\n",
    "        \n",
    "        #sample_init[np.logical_not(condition.astype(bool))] = 0\n",
    "        sample_zero = sample_init.copy()\n",
    "        sample_init[abs(sample_init) < 1] = 0\n",
    "        ###### save the  raw data #######\n",
    "        data_df[f'r_for_{index}'] = sample_init[0:20]\n",
    "        data_df[f'r_bac_{index}'] = sample_init[20:40]\n",
    "        data_df[f'r_bus_{index}'] = pd.Series(sample_init[40:].tolist())\n",
    "\n",
    "        # Saving the raw data\n",
    "        column_name = f'{index}_0'\n",
    "        sample_df = pd.DataFrame(sample_init.tolist(), index = range(1, numOfZ+1), columns = [column_name])\n",
    "        #false_Data_Gen = false_Data_Gen.append(sample_df.T,  ignore_index = False)\n",
    "\n",
    "        ###########################\n",
    "        sample_mean = np.mean(sample_init)\n",
    "        sample_std = np.std(sample_init)\n",
    "        #print(\"Mean and Std:\", sample_mean, sample_std)\n",
    "\n",
    "        # repeating for delZ_th_list\n",
    "        for std_index, std_fact in enumerate(std_factors):\n",
    "            \n",
    "            dict_data = {}\n",
    "            #print(\"\\nstd_fact: \", std_fact)\n",
    "            #setting up the threshold\n",
    "            delZ_th = sample_std*std_fact\n",
    "            #print(\"std_fact, delZ_th :\", std_fact, delZ_th)\n",
    "            \n",
    "            # Repeating for muliple times\n",
    "            for est in range(max_est+1):\n",
    "\n",
    "                #print(\"Estimation: \", est, max_est)\n",
    "\n",
    "                # Senitize the samples\n",
    "#                 if run_harmonizer == True:\n",
    "                if max_est > 0:\n",
    "                    Z_mat[1:, 2] = sample_init.copy()\n",
    "                    States_org, Z_est_org, fullRank_org = attack_estimate(H_org.copy(), Z_mat.copy())\n",
    "                    sample = Z_est_org[1:].copy()\n",
    "            #         if est == 0:\n",
    "                    dis = mean_absolute_error(sample_zero, Z_est_org[1:])\n",
    "                        #dis  = np.mean(abs(sample_zero - Z_est_org[1:])) #/np.linalg.norm(Z_est_org[1:])*100\n",
    "                else:\n",
    "                    sample = sample_init.copy()\n",
    "                    dis = 0\n",
    "\n",
    "                #Removing noises\n",
    "                sample[abs(sample)<1] = 0\n",
    "                \n",
    "                #removing unaccessible injection\n",
    "                #*****************************************************************************\n",
    "                #sample[np.logical_not(condition.astype(bool))] = 0\n",
    "                \n",
    "                # Replacing with 0\n",
    "                sample[abs(sample)<delZ_th] = 0\n",
    "\n",
    "                ### Verifing the attack stealthyness\n",
    "                Z_mat[1:, 2] = Z_org[1:, 2] + sample\n",
    "                W_list = (numOfZ + 1)*[1]\n",
    "\n",
    "                # estimate the attack vectors\n",
    "                try:\n",
    "                    States_init, Z_est_init, Z_mat_init, M_Noise_actu, Noisy_index_actu, fullRank, Threshold = SE_BDD_(\n",
    "                    H_org.copy(), Z_mat.copy(), W_list, Threshold_min , Threshold_max , Verbose = \"False\")\n",
    "                    #print(f\"\\nStep: {est}: Noise: \", Noisy_index_actu, \"Threshold:\", Threshold, \"fullRank:\", fullRank)\n",
    "    #                 #save the noisy sensors in the fist estimation \n",
    "    #                 if est == 0: noisy_preds.append(Noisy_index_actu.size)  \n",
    "\n",
    "                except:\n",
    "                    print(\"Error in estiamtion\")\n",
    "                    break\n",
    "\n",
    "\n",
    "                #final_sample = sample.copy()\n",
    "\n",
    "                if  fullRank == False or np.linalg.norm(sample[0:2*numOfLines]) == 0: #abs(final_sample).max()> 1000 or\n",
    "#                     print(f\"\\nStep: {est}: Noise: \", Noisy_index_actu, \"Threshold:\", Threshold, \"fullRank:\", fullRank)\n",
    "#                     print(\"abs(sample).max()\", abs(sample).max(), sample.argmax())\n",
    "                    break\n",
    "\n",
    "                    ###########################\n",
    "\n",
    "                if Noisy_index_actu.size == 0:\n",
    "\n",
    "                    distance.append(dis)\n",
    "    #                 progress_mini.update(1)\n",
    "                    success[std_index] += 1\n",
    "\n",
    "                    #################### saving as the clean data \n",
    "                    data_df[f'cf_{index}_{std_fact}'] = sample[0:20]\n",
    "                    data_df[f'cb_{index}_{std_fact}'] = sample[20:40]\n",
    "                    data_df[f'cc_{index}_{std_fact}'] = pd.Series(sample[40:].tolist())\n",
    "                    ###################\n",
    "\n",
    "                    ###########################\n",
    "                    column_name = f'{index}_{std_index+1}'\n",
    "                    sample_df = pd.DataFrame(sample.tolist(), index = range(1, numOfZ+1), columns = [column_name])\n",
    "                    false_Data_Gen = false_Data_Gen.append(sample_df.T,  ignore_index = False)\n",
    "                    \n",
    "                    ###########################\n",
    "                    dict_data['index'] = index\n",
    "                    dict_data['std_index'] = std_index\n",
    "                    dict_data['max_est'] = max_est\n",
    "                    dict_data['impact'] = np.linalg.norm(sample)\n",
    "                    dict_data['sample'] = sample.tolist()\n",
    "                    \n",
    "                    #dict_data['std_index'] = std_index\n",
    "                    final_Data = pd.concat([final_Data, pd.DataFrame(dict_data.values()).T], axis = 0)\n",
    "\n",
    "                    ###########################\n",
    "                    #print(\"Clean\")\n",
    "    #                 ###########################\n",
    "                    break\n",
    "\n",
    "    #             sample_init = Z_est_init[1:].copy() \n",
    "                #print(\"Repeating \", Noisy_index_actu.size)\n",
    "                sample_init = sample.copy()   \n",
    "\n",
    "                #saving sample and going for the next run\n",
    "    #             sample = Z_est_init[1:].copy()\n",
    "\n",
    "    false_Data_Gen_Est.append(false_Data_Gen)\n",
    "    success_rates.append(success)\n",
    "print(\"Done\")\n",
    "final_Data.columns = ['index', 'std_index', 'max_est', 'impact', 'sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (4.5,4))\n",
    "fnt= '17.5'\n",
    "markers=['v','o','<','>','*','o','s','v','o','s']\n",
    "linestyles=['--','-.','-.','--','-.','-','-.','-','--','-.','-']\n",
    "for i,y in enumerate(np.array(success_rates)):\n",
    "    if i == -1:\n",
    "        continue\n",
    "    plt.plot(std_factors, y/samples.shape[0]*100, marker = markers[i], linestyle = linestyles[i], markersize = '7.5', label = max_est_list[i])\n",
    "plt.title(f\"{machine} with Different $\\gamma$\", fontsize = fnt)\n",
    "plt.xlabel(\"Threshold Factor, $\\lambda$\", fontsize = fnt)\n",
    "plt.xticks(std_factors, fontsize = fnt)\n",
    "plt.yticks(fontsize = fnt)\n",
    "plt.ylabel(\"SR (%)\",  fontsize = fnt)\n",
    "# plt.legend(fontsize = fnt, framealpha = 0.5)\n",
    "plt.legend(loc='upper right', ncol = 2, borderaxespad=0., fontsize = '13.5', framealpha = 0.0) #bbox_to_anchor=(1.02, 0.75)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Evaluation//'+Model+f'//{numOfBuses}//SR_{machine}_{attacked_Bus}.jpg', dpi=350)\n",
    "plt.savefig('Evaluation//'+Model+f'//{numOfBuses}//SR_{machine}_{attacked_Bus}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if machine == 'RAND':\n",
    "    plt.figure(figsize = (4.5,4))\n",
    "\n",
    "    fnt= '17.5'\n",
    "    markers=['v','o','<','>','*','o','s','v','o','s']\n",
    "    linestyles=['--','-.','-.','--','-.','-','-.','-','--','-.','-']\n",
    "    for i,y in enumerate(np.array(success_rates)):\n",
    "        if i == -1:\n",
    "            continue\n",
    "        plt.plot(std_factors, y/samples.shape[0]*100, marker = markers[i], linestyle = linestyles[i], markersize = '7.5', label = max_est_list[i])\n",
    "    plt.title(f\"Random Injections with Different $\\gamma$\", fontsize = fnt)\n",
    "    plt.xlabel(\"Threshold Factor, $\\lambda$\", fontsize = fnt)\n",
    "    plt.xticks(std_factors, fontsize = fnt)\n",
    "    plt.yticks(fontsize = fnt)\n",
    "    plt.ylabel(\"SR (%)\",  fontsize = fnt)\n",
    "    # plt.legend(fontsize = fnt, framealpha = 0.5)\n",
    "    plt.legend(loc='best', ncol = 2, borderaxespad=0., fontsize = '15', framealpha = 0.5) #bbox_to_anchor=(1.02, 0.95),\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Evaluation//'+Model+f'//{numOfBuses}//SR_{machine}_{attacked_Bus}.jpg', dpi=350)\n",
    "    plt.savefig('Evaluation//'+Model+f'//{numOfBuses}//SR_{machine}_{attacked_Bus}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_Data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter1 = final_Data['max_est'] == 5\n",
    "# filter2 = final_Data['std_index'] == std_index\n",
    "filtered_Data = final_Data.where(filter1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_Data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexList = []\n",
    "for i, inde in enumerate(range(max(filtered_Data['index']))):\n",
    "    filter1 = filtered_Data['index'] == inde\n",
    "    if filtered_Data.where(filter1).dropna().shape[0] == 5:\n",
    "        indexList.append(inde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot = []\n",
    "nonzero = []\n",
    "for x in filtered_Data.values:\n",
    "    if x[0] in indexList:\n",
    "        data_plot.append(x)\n",
    "        y = np.array(x[-1])\n",
    "        y[y!=0] = 1\n",
    "        nonzero.append(np.sum(y))\n",
    "data_plot = pd.DataFrame(data_plot)\n",
    "data_plot.columns = final_Data.columns\n",
    "data_plot['compSen'] = nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.linalg.norm(false_Data_Train, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4.5,4))\n",
    "target = 'generated'\n",
    "ax = sns.kdeplot(np.linalg.norm(false_Data_Train, axis = 1), shade = True, label = '$\\mathcal{T}_F$', clip=(0, 150), linestyle = '--')\n",
    "\n",
    "for i in range(len(std_factors)):\n",
    "    filterr = data_plot['std_index'] == i\n",
    "    data_temp = data_plot.where(filterr).dropna().copy()\n",
    "    ax = sns.kdeplot(data_temp['impact'], shade = True, label = f'$\\lambda=$ {std_factors[i]}')\n",
    "\n",
    "plt.legend(loc = 'upper right',fontsize = '15')\n",
    "plt.title('Distribution of Attack Impact',fontsize = fnt)\n",
    "plt.xlabel(\"Attack Impact\", fontsize = fnt)\n",
    "plt.xticks(fontsize = '15')\n",
    "plt.yticks(fontsize = '15')\n",
    "plt.legend( fontsize = '15')\n",
    "plt.ylabel(\"Probability\",fontsize = fnt)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Evaluation//'+Model+f'//{numOfBuses}//{Model}_injection.jpg', dpi = 350)\n",
    "plt.savefig('Evaluation//'+Model+f'//{numOfBuses}//{Model}_injection.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_factors \n",
    "data_plot['compSen']= data_plot['compSen']/numOfZ*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4.5,4))\n",
    "\n",
    "#sns.kdeplot(combination_Data_Train.T.describe().loc['mean']*100, shade=True, label = '$\\mathcal{T}_F$')\n",
    "\n",
    "\n",
    "for i in range(len(std_factors)):\n",
    "    filterr = data_plot['std_index'] == i\n",
    "    data_temp = data_plot.where(filterr).dropna().copy()\n",
    "    #print(data_temp['compSen'])\n",
    "    ax = sns.kdeplot(data_temp['compSen'], shade=True,  label = f'$\\lambda=$ {std_factors[i]}')\n",
    "#     plt.legend()\n",
    "\n",
    "plt.xlabel(\"Compormised Sensors (%)\", fontsize = fnt)\n",
    "plt.xticks(fontsize = '15')\n",
    "plt.yticks(fontsize = '15')\n",
    "plt.legend( loc = 'upper right', fontsize = '15')\n",
    "plt.ylabel(\"Probability\",fontsize = fnt)\n",
    "# plt.title('Distribution of Compromised Sensors in '+target+' data', fontsize = '15')\n",
    "plt.title('Dist. of Compromised Sensors', fontsize = fnt)\n",
    "\n",
    "\n",
    "ax2=ax.twinx()\n",
    "ax2 = sns.kdeplot(combination_Data_Train.T.describe().loc['mean']*100, shade=True, label = '$\\mathcal{T}_F$')\n",
    "# ax2.legend(\"Probability (training data)\")\n",
    "plt.legend( loc = 'lower right', fontsize = '15', framealpha = 0.25)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Evaluation//'+Model+f'//{numOfBuses}//{Model}_attacked_sensors.jpg', dpi = 350)\n",
    "plt.savefig('Evaluation//'+Model+f'//{numOfBuses}//{Model}_attacked_sensors.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
